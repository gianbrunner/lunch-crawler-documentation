\chapter{Stand der Forschung}
\label{chap:forschung}
Diese Arbeit stützt sich auf verschiedene Bücher, Websites und Artikel.
Die Grundlagen eines Webcrawlers wurden mit Hilfe des Buchs \glqq Web Data Mining\grqq{} Kapitel 8 erkannt \cite[p. 311ff.]{liu2007web}. 
Darin wird die Vorgehensweise eines Webcrawlers erläutert und zudem diverse bekannte Implementierungsprobleme wie Link-Loops.
Es wird auf die Etiquette verwiesen, wie z.B. das Berücksichtigen von Robots.txt Dateien, welche definieren, ob eine Website gecrawlt werden darf oder nicht\cite[p. 353ff.]{liu2007web}.
Auch die Anzahl Anfragen derselben Website soll limitiert werden, um diese nicht zu überlasten\cite[p. 353ff.]{liu2007web}.\\
Die Umsetzung des Webcrawlers mit StormCrawler SDK wurde hauptsächlich durch die Website\cite{StormCrawler} und das Wiki des Erstellers\cite{GithubStormCrawler}, Julien Nioche, erarbeitet.\\
Da Stormcrawler auf Apache Storm basiert, sind die Prinzipien dieser Technologie ebenfalls relevant.
In einen Artikel von Usama Ashraf werden die grundlegenden Komponenten, deren Funktionen und wie man sie verwendet, erklärt.
Zudem wird erläutert, wie man eine Storm-Topologie mittels Docker-Compose betreibt\cite{ApacheStormDev}.\\
% Verweise ML Algorithmen und No Free Lunch Theorem
% Effizienter Crawler mittels Apache Storm
% CISTEM erwähnen
% Pemistahl Lingua erwähnen