\chapter{Schlusswort}
\section{Erkenntnisse}
Während der Durchführung dieser Arbeit wurden wichtige Erkenntnisse gewonnen.
Eine Evaluation des Webcrawlers wäre von Vorteil gewesen.
StormCrawler hat zwar die Funktion erfüllt, dieser ist jedoch dafür ausgelegt, eine enorme Masse an Websites und Webpages zu crawlen.
Dies ist jedoch keine Hauptanforderung in dieser Arbeit gewesen.
Dafür wäre ein Webcrawler, welcher auch dynamisch generierte Websites handhaben kann, von Vorteil gewesen.
Für die Klassifizierung hätte eine Preprocessing-Methode zur Erkennung des Hauptinhalts einer Webpage einen grossen Benefit bewirkt.
Beim manuellen Labeling wurde erkannt, dass viele Webpages nicht relevante oder sogar irreführende Informationen im Kopf- und Fussteil beinhalten.
Der Gold Standard sollte ein zweites Mal gelabelt werden, damit dieser nach dem Vier-Augen-Prinzip kontrolliert werden würde.
Auf diesen Arbeitsschritt wurde jedoch aus zeitlichen Gründen abgesehen.
Um die regelbasierte Klassifikation zu verbessern, wären weitere Regeln sowie das Erweitern und Anpassen der Black- und Whitelist sinnvoll.
Bei der Klassifikation mittels Machine-Learning wurde erkannt, dass das \glqq No-Free-Lunch\grqq{}-Theorem nicht ohne Grund aufgestellt worden ist.
Das Entwicklen von Machine-Learning Anwendungen folgt dem Prinzip \glqq Try-and-Error\grqq{} und bietet eine Vielzahl von Stolpersteinen.
Um die Performance der Klassifizierer zu verbessern, wäre die Verwendung von anderen textbasierten Feature-Extraction Methoden ein möglicher Ansatz.
Ebenfalls könnte die Grösse der Trainingsdaten erhöht werden, um den Algorithmen die Möglichkeit zu geben, mehr relevante Zusammenhänge aus den Informationen zu entnehmen.
\section{Ausblick}
Als weiterer Schritt wäre eine Automatisierung wichtig.
Darunter versteht sich einerseits das Automatisieren des Webcrawlers, also eine kontinuierliche Erweiterung des Seeds sowie eine regelmässige Durchführung des Crawldurchlaufs.
Andererseits müssen die einzelnen Komponenten untereinander mittels definierter Schnittstellen verbunden und automatisiert werden, sodass gecrawlte Daten ohne manuellen Input klassifiziert und für die Webapplikation bereitgestellt werden.\\
Im Zusammenhang mit der Automatisierung muss auch die Performance optimiert werden.
Um beispielsweise tägliche Mittagsmenüs erfassen zu können, sollte ein Crawldurchlauf innerhalb weniger Stunden durchgeführt werden.\\
Für die Klassifikation mittels Machine-Learning wäre das Verwenden von Deep-Learning Ansätzen ebenfalls eine Alternative.
Deep-Learning Algorithmen können bei Klassifizierungsaufgaben sehr gute Ergebnisse erzielen, sofern jedoch genügend Trainingsdaten vorhanden sind.\\
Zusätzlich wäre eine Erweiterung der Webapplikation mit zusätzlichen Features, wie die Angabe eines maximalen Suchradius oder die Suche an einer bestimmten Postleitzahl, möglich.
Ebenfalls könnte die Webapplikation ansprechender und intuitiver gestaltet werden.
 

